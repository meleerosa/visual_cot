import torch
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from qwen_vl_utils import process_vision_info

# 1. ì„¤ì •
MERGED_DIR = "/root/project/step1-vgjson-grounding-ve-merged"   # merge_and_unload() í›„ ì €ì¥í•œ ë””ë ‰í† ë¦¬
IMG_PATH   = "/root/project/data/visual_genome/VG_100K/2.jpg"
PROMPT     = "Describe this image."
device     = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2. Processor (tokenizer + vision prep) ë¶ˆëŸ¬ì˜¤ê¸°
processor = AutoProcessor.from_pretrained(
    MERGED_DIR,
    trust_remote_code=True,
    use_fast=True,      # special tokens ì•ˆì •ì  ë¡œë“œë¥¼ ìœ„í•´ slow tokenizer ì‚¬ìš©
)
processor.tokenizer.pad_token = processor.tokenizer.eos_token

# 3. ë³‘í•©ëœ ëª¨ë¸ ê·¸ëŒ€ë¡œ ë¡œë“œ
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    MERGED_DIR,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
model.to(device).eval()

# 4. Inference ì…ë ¥ ì¤€ë¹„
image = Image.open(IMG_PATH).convert("RGB").resize((448, 448))
messages = [{
    "role": "user",
    "content": [
        {"type": "image", "image": image},
        {"type": "text",  "text": PROMPT},
    ],
}]

# chat template ë Œë”ë§ & vision info ì²˜ë¦¬
text_prompt    = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
vision_inputs, video_inputs = process_vision_info(messages)

inputs = processor(
    text=[text_prompt],
    images=vision_inputs,
    videos=video_inputs,
    return_tensors="pt",
    padding=True,
).to(device)

# 5. Generate
with torch.no_grad():
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=True,             # ìƒ˜í”Œë§ ëª¨ë“œë¡œ ë°”ê¿”ì„œ ë‹¤ì–‘ì„±â†‘
        temperature=0.9,            # ìƒì„± ì˜¨ë„ ì¡°ì ˆ
        top_p=0.9,                  # nucleus sampling
        repetition_penalty=1.1,     # ë°˜ë³µ ì–µì œ
        no_repeat_ngram_size=3,     # 3-gram ì´ìƒ ë°˜ë³µ ê¸ˆì§€
        eos_token_id=processor.tokenizer.eos_token_id
    )

# 6. ë””ì½”ë”© & ì¶œë ¥
trimmed = [
    out_ids[len(in_ids):]
    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output = processor.tokenizer.batch_decode(
    trimmed,
    skip_special_tokens=False
)[0]
for tok in ["[objects]", "[/objects]", "[answer]", "[/answer]"]:
    tid = processor.tokenizer.convert_tokens_to_ids(tok)
    emb = model.get_input_embeddings()(torch.tensor([tid]).to(model.device))
    print(f"{tok} | norm: {emb.norm().item():.4f}")

print("ğŸ“ Model Output:\n", output)
